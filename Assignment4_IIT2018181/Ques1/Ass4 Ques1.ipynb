{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the files\n",
    "import numpy as np\n",
    "import pandas as pda\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import matplotlib.pyplot as plt\n",
    "import code as cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the csv file with the help of pandas\n",
    "examData = pda.read_csv('Two_Exam_Result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet,testingSet = tts(examData,test_size=0.3,random_state= 81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainingX = trainingSet[examData.columns[0:2]].copy()\n",
    "trainingY = trainingSet['Pass/Fail'].copy()\n",
    "testingX = testingSet[examData.columns[0:2]].copy()\n",
    "testingY = testingSet['Pass/Fail'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting them to numpy\n",
    "trainingX = trainingX.to_numpy()\n",
    "trainingY = trainingY.to_numpy()\n",
    "testingX = testingX.to_numpy()\n",
    "testingY = testingY.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenating the column with one from front\n",
    "trainingX = np.concatenate([np.ones((trainingX.shape[0] ,1)), trainingX], axis=1)\n",
    "testingX = np.concatenate([np.ones((testingX.shape[0], 1)), testingX], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################## NORMAL SCENARIO ####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****************** This is Part(a) ************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Batch Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(trainingX.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "costList,theta = cd.batch_gradient_descent(trainingX,trainingY, theta, 2000, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Gradient Descent Algorithm Without Feature Scaling\n",
      "w0:  -0.001336483737936671\n",
      "w1:  0.010438702130810381\n",
      "w2:  0.0005547592695009956\n",
      "Error:  40.0\n",
      "Accuracy:  60.0\n"
     ]
    }
   ],
   "source": [
    "z = testingX.dot(theta)\n",
    "predictedY = 1.0/(1.0 + np.exp(-z))\n",
    "predictedY = np.round(predictedY)\n",
    "difference = np.abs(np.subtract(predictedY,testingY))\n",
    "temp = np.sum(difference)\n",
    "temp = temp/predictedY.shape[0]\n",
    "error = np.multiply(temp,100)\n",
    "print('Batch Gradient Descent Algorithm Without Feature Scaling')\n",
    "print('w0: ', theta[0])\n",
    "print('w1: ', theta[1])\n",
    "print('w2: ', theta[2])\n",
    "print('Error: ', error)\n",
    "accuracy = 100 - error\n",
    "print('Accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Stochastic Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(trainingX.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "costList,theta = cd.stochastic_gradient_descent(trainingX,trainingY, theta, 2000, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent Algorithm Without Feature Scaling\n",
      "w0:  -0.0013376817824654666\n",
      "w1:  0.010410210445260302\n",
      "w2:  0.0003754232931499535\n",
      "Error:  40.0\n",
      "Accuracy:  60.0\n"
     ]
    }
   ],
   "source": [
    "z = testingX.dot(theta)\n",
    "predictedY = 1.0/(1.0 + np.exp(-z))\n",
    "predictedY = np.round(predictedY)\n",
    "difference = np.abs(np.subtract(predictedY,testingY))\n",
    "temp = np.sum(difference)\n",
    "temp = temp/predictedY.shape[0]\n",
    "error = np.multiply(temp,100)\n",
    "print('Stochastic Gradient Descent Algorithm Without Feature Scaling')\n",
    "print('w0: ', theta[0])\n",
    "print('w1: ', theta[1])\n",
    "print('w2: ', theta[2])\n",
    "print('Error: ', error)\n",
    "accuracy = 100 - error\n",
    "print('Accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Mini Batch Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(trainingX.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "costList,theta = cd.mini_batch_gradient_descent(trainingX,trainingY, theta, 2000, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini Batch Gradient Descent Algorithm Without Feature Scaling\n",
      "w0:  -0.0013364357024045841\n",
      "w1:  0.010454707109610413\n",
      "w2:  0.0005612848549127861\n",
      "Error:  40.0\n",
      "Accuracy:  60.0\n"
     ]
    }
   ],
   "source": [
    "z = testingX.dot(theta)\n",
    "predictedY = 1.0/(1.0 + np.exp(-z))\n",
    "predictedY = np.round(predictedY)\n",
    "difference = np.abs(np.subtract(predictedY,testingY))\n",
    "temp = np.sum(difference)\n",
    "temp = temp/predictedY.shape[0]\n",
    "error = np.multiply(temp,100)\n",
    "print('Mini Batch Gradient Descent Algorithm Without Feature Scaling')\n",
    "print('w0: ', theta[0])\n",
    "print('w1: ', theta[1])\n",
    "print('w2: ', theta[2])\n",
    "print('Error: ', error)\n",
    "accuracy = 100 - error\n",
    "print('Accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### WITH FEATURE SCALING ####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are doing min max scaling for trainingX and testingX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingXF = trainingX\n",
    "testingXF = testingX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini = np.min(trainingXF,axis=0)\n",
    "maxi = np.max(trainingXF,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingXF[:, 1:] = (trainingXF[:, 1:] - mini[1:])/(maxi[1:] - mini[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini = np.min(testingXF,axis=0)\n",
    "maxi = np.max(testingXF,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingXF[:, 1:] = (testingXF[:, 1:] - mini[1:])/(maxi[1:] - mini[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Batch Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(trainingXF.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "costList,theta = cd.batch_gradient_descent(trainingXF,trainingY, theta, 2000, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Gradient Descent Algorithm With Feature Scaling\n",
      "w0:  0.1109616507675669\n",
      "w1:  0.21351506653865082\n",
      "w2:  0.1985361668314663\n",
      "Error:  40.0\n",
      "Accuracy:  60.0\n"
     ]
    }
   ],
   "source": [
    "z = testingXF.dot(theta)\n",
    "predictedY = 1.0/(1.0 + np.exp(-z))\n",
    "predictedY = np.round(predictedY)\n",
    "difference = np.abs(np.subtract(predictedY,testingY))\n",
    "temp = np.sum(difference)\n",
    "temp = temp/predictedY.shape[0]\n",
    "error = np.multiply(temp,100)\n",
    "print('Batch Gradient Descent Algorithm With Feature Scaling')\n",
    "print('w0: ', theta[0])\n",
    "print('w1: ', theta[1])\n",
    "print('w2: ', theta[2])\n",
    "print('Error: ', error)\n",
    "accuracy = 100 - error\n",
    "print('Accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Stochastic Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(trainingXF.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "costList,theta = cd.stochastic_gradient_descent(trainingXF,trainingY, theta, 2000, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent Algorithm With Feature Scaling\n",
      "w0:  0.0019911593504501834\n",
      "w1:  0.0026260984610931492\n",
      "w2:  0.0024746830030753122\n",
      "Error:  40.0\n",
      "Accuracy:  60.0\n"
     ]
    }
   ],
   "source": [
    "z = testingXF.dot(theta)\n",
    "predictedY = 1.0/(1.0 + np.exp(-z))\n",
    "predictedY = np.round(predictedY)\n",
    "difference = np.abs(np.subtract(predictedY,testingY))\n",
    "temp = np.sum(difference)\n",
    "temp = temp/predictedY.shape[0]\n",
    "error = np.multiply(temp,100)\n",
    "print('Stochastic Gradient Descent Algorithm With Feature Scaling')\n",
    "print('w0: ', theta[0])\n",
    "print('w1: ', theta[1])\n",
    "print('w2: ', theta[2])\n",
    "print('Error: ', error)\n",
    "accuracy = 100 - error\n",
    "print('Accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Mini Batch Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(trainingXF.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "costList,theta = cd.mini_batch_gradient_descent(trainingXF,trainingY, theta, 2000, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini Batch Gradient Descent Algorithm With Feature Scaling\n",
      "w0:  0.0019886230890000106\n",
      "w1:  0.0026267215076046435\n",
      "w2:  0.00246679770399935\n",
      "Error:  40.0\n",
      "Accuracy:  60.0\n"
     ]
    }
   ],
   "source": [
    "z = testingXF.dot(theta)\n",
    "predictedY = 1.0/(1.0 + np.exp(-z))\n",
    "predictedY = np.round(predictedY)\n",
    "difference = np.abs(np.subtract(predictedY,testingY))\n",
    "temp = np.sum(difference)\n",
    "temp = temp/predictedY.shape[0]\n",
    "error = np.multiply(temp,100)\n",
    "print('Mini Batch Gradient Descent Algorithm With Feature Scaling')\n",
    "print('w0: ', theta[0])\n",
    "print('w1: ', theta[1])\n",
    "print('w2: ', theta[2])\n",
    "print('Error: ', error)\n",
    "accuracy = 100 - error\n",
    "print('Accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ WITH REGULARIZATION ##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************************** This is Part (c) *****************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Batch Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(trainingX.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "costList,theta = cd.batch_gradient_descent_reg(trainingX,trainingY, theta, 2000, 0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent Algorithm With Regularization\n",
      "w0:  1.997145862650097e-07\n",
      "w1:  2.6292756342159163e-07\n",
      "w2:  2.4694553586632616e-07\n",
      "Error:  40.0\n",
      "Accuracy:  60.0\n"
     ]
    }
   ],
   "source": [
    "z = testingX.dot(theta)\n",
    "predictedY = 1.0/(1.0 + np.exp(-z))\n",
    "predictedY = np.round(predictedY)\n",
    "difference = np.abs(np.subtract(predictedY,testingY))\n",
    "temp = np.sum(difference)\n",
    "temp = temp/predictedY.shape[0]\n",
    "error = np.multiply(temp,100)\n",
    "print('Stochastic Gradient Descent Algorithm With Regularization')\n",
    "print('w0: ', theta[0])\n",
    "print('w1: ', theta[1])\n",
    "print('w2: ', theta[2])\n",
    "print('Error: ', error)\n",
    "accuracy = 100 - error\n",
    "print('Accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Stochastic Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(trainingX.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "costList,theta = cd.stochastic_gradient_descent_reg(trainingX,trainingY, theta, 2000, 0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent Algorithm With Regularization\n",
      "w0:  1.801953361325639e-07\n",
      "w1:  2.381469526649709e-07\n",
      "w2:  2.237039117774259e-07\n",
      "Error:  40.0\n",
      "Accuracy:  60.0\n"
     ]
    }
   ],
   "source": [
    "z = testingX.dot(theta)\n",
    "predictedY = 1.0/(1.0 + np.exp(-z))\n",
    "predictedY = np.round(predictedY)\n",
    "difference = np.abs(np.subtract(predictedY,testingY))\n",
    "temp = np.sum(difference)\n",
    "temp = temp/predictedY.shape[0]\n",
    "error = np.multiply(temp,100)\n",
    "print('Stochastic Gradient Descent Algorithm With Regularization')\n",
    "print('w0: ', theta[0])\n",
    "print('w1: ', theta[1])\n",
    "print('w2: ', theta[2])\n",
    "print('Error: ', error)\n",
    "accuracy = 100 - error\n",
    "print('Accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Mini Batch Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(trainingX.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "costList,theta = cd.mini_batch_gradient_descent_reg(trainingX,trainingY, theta, 2000, 0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini Batch Gradient Descent Algorithm With Regularization\n",
      "w0:  1.9801373892749593e-07\n",
      "w1:  2.606879471222281e-07\n",
      "w2:  2.448421719604039e-07\n",
      "Error:  40.0\n",
      "Accuracy:  60.0\n"
     ]
    }
   ],
   "source": [
    "z = testingX.dot(theta)\n",
    "predictedY = 1.0/(1.0 + np.exp(-z))\n",
    "predictedY = np.round(predictedY)\n",
    "difference = np.abs(np.subtract(predictedY,testingY))\n",
    "temp = np.sum(difference)\n",
    "temp = temp/predictedY.shape[0]\n",
    "error = np.multiply(temp,100)\n",
    "print('Mini Batch Gradient Descent Algorithm With Regularization')\n",
    "print('w0: ', theta[0])\n",
    "print('w1: ', theta[1])\n",
    "print('w2: ', theta[2])\n",
    "print('Error: ', error)\n",
    "accuracy = 100 - error\n",
    "print('Accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## FEATURE ADDING #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********************* This is Part (b) **************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "addTraining = []\n",
    "addTesting = []\n",
    "\n",
    "for i in range(len(trainingX)):\n",
    "    addTraining.append([trainingX[i][1]**2,trainingX[i][2]**2,trainingX[i][1]*trainingX[i][2],(trainingX[i][1])*(trainingX[i][2]**2),(trainingX[i][1]**2)*(trainingX[i][2]),(trainingX[i][1]**2)*(trainingX[i][2]**2)])\n",
    "for i in range(len(testingX)):\n",
    "    addTesting.append([testingX[i][1]**2,testingX[i][2]**2,testingX[i][1]*testingX[i][2],(testingX[i][1])*(testingX[i][2]**2),(testingX[i][1]**2)*(testingX[i][2]),(testingX[i][1]**2)*(testingX[i][2]**2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingX = np.concatenate((trainingX,addTraining),axis=1)\n",
    "testingX = np.concatenate((testingX,addTesting),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************** Using Part(b) in Part (a) ***************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Batch Gradient Descent Algorithm with feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(trainingX.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "costList,theta = cd.batch_gradient_descent(trainingX,trainingY, theta, 2000, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Gradient Descent Algorithm After Adding Features\n",
      "w0:  0.0019823251028635697\n",
      "w1:  0.0026225588749339643\n",
      "w2:  0.0024627010618741537\n",
      "w3:  0.0021115989748732513\n",
      "w4:  0.0019664901707631684\n",
      "w5:  0.0020718559098144096\n",
      "w6:  0.0015206237052825793\n",
      "w7:  0.001520901068155705\n",
      "w8:  0.0010774122043876971\n",
      "Error:  40.0\n",
      "Accuracy:  60.0\n"
     ]
    }
   ],
   "source": [
    "z = testingX.dot(theta)\n",
    "predictedY = 1.0/(1.0 + np.exp(-z))\n",
    "predictedY = np.round(predictedY)\n",
    "difference = np.abs(np.subtract(predictedY,testingY))\n",
    "temp = np.sum(difference)\n",
    "temp = temp/predictedY.shape[0]\n",
    "error = np.multiply(temp,100)\n",
    "print('Batch Gradient Descent Algorithm After Adding Features')\n",
    "print('w0: ', theta[0])\n",
    "print('w1: ', theta[1])\n",
    "print('w2: ', theta[2])\n",
    "print('w3: ', theta[3])\n",
    "print('w4: ', theta[4])\n",
    "print('w5: ', theta[5])\n",
    "print('w6: ', theta[6])\n",
    "print('w7: ', theta[7])\n",
    "print('w8: ', theta[8])\n",
    "print('Error: ', error)\n",
    "accuracy = 100 - error\n",
    "print('Accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Stochastic Gradient Descent Algorithm with feature scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(trainingX.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "costList,theta = cd.stochastic_gradient_descent(trainingX,trainingY, theta, 2000, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini Batch Gradient Descent Algorithm After Adding Features\n",
      "w0:  0.0019614748126276626\n",
      "w1:  0.002608929987328561\n",
      "w2:  0.0024510405221463373\n",
      "w3:  0.0020982134085014725\n",
      "w4:  0.0019583819511803506\n",
      "w5:  0.0020623520892111575\n",
      "w6:  0.0015140989808311233\n",
      "w7:  0.0015116808859920398\n",
      "w8:  0.001071111258857712\n",
      "Error:  40.0\n",
      "Accuracy:  60.0\n"
     ]
    }
   ],
   "source": [
    "z = testingX.dot(theta)\n",
    "predictedY = 1.0/(1.0 + np.exp(-z))\n",
    "predictedY = np.round(predictedY)\n",
    "difference = np.abs(np.subtract(predictedY,testingY))\n",
    "temp = np.sum(difference)\n",
    "temp = temp/predictedY.shape[0]\n",
    "error = np.multiply(temp,100)\n",
    "print('Mini Batch Gradient Descent Algorithm After Adding Features')\n",
    "print('w0: ', theta[0])\n",
    "print('w1: ', theta[1])\n",
    "print('w2: ', theta[2])\n",
    "print('w3: ', theta[3])\n",
    "print('w4: ', theta[4])\n",
    "print('w5: ', theta[5])\n",
    "print('w6: ', theta[6])\n",
    "print('w7: ', theta[7])\n",
    "print('w8: ', theta[8])\n",
    "print('Error: ', error)\n",
    "accuracy = 100 - error\n",
    "print('Accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Mini Batch Gradient Descent Algorithm with Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(trainingX.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "costList,theta = cd.mini_batch_gradient_descent(trainingX,trainingY, theta, 2000, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini Batch Gradient Descent Algorithm After Adding Features\n",
      "w0:  0.0019823193910287577\n",
      "w1:  0.002622555075918062\n",
      "w2:  0.002462697830475231\n",
      "w3:  0.002111596061196059\n",
      "w4:  0.0019664879261666545\n",
      "w5:  0.002071853773961573\n",
      "w6:  0.0015206222417996323\n",
      "w7:  0.0015208994454348644\n",
      "w8:  0.0010774111094513509\n",
      "Error:  40.0\n",
      "Accuracy:  60.0\n"
     ]
    }
   ],
   "source": [
    "z = testingX.dot(theta)\n",
    "predictedY = 1.0/(1.0 + np.exp(-z))\n",
    "predictedY = np.round(predictedY)\n",
    "difference = np.abs(np.subtract(predictedY,testingY))\n",
    "temp = np.sum(difference)\n",
    "temp = temp/predictedY.shape[0]\n",
    "error = np.multiply(temp,100)\n",
    "print('Mini Batch Gradient Descent Algorithm After Adding Features')\n",
    "print('w0: ', theta[0])\n",
    "print('w1: ', theta[1])\n",
    "print('w2: ', theta[2])\n",
    "print('w3: ', theta[3])\n",
    "print('w4: ', theta[4])\n",
    "print('w5: ', theta[5])\n",
    "print('w6: ', theta[6])\n",
    "print('w7: ', theta[7])\n",
    "print('w8: ', theta[8])\n",
    "print('Error: ', error)\n",
    "accuracy = 100 - error\n",
    "print('Accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********************* Using Part (b) in Part (c) ***************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Batch Gradient Descent Algorithm with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(trainingX.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "costList,theta = cd.batch_gradient_descent_reg(trainingX,trainingY, theta, 2000, 0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Gradient Descent Algorithm After Adding Features with Regularization\n",
      "w0:  1.9971452305930438e-07\n",
      "w1:  2.6292752166884145e-07\n",
      "w2:  2.4694549477836577e-07\n",
      "w3:  2.11614112290559e-07\n",
      "w4:  1.971047136398958e-07\n",
      "w5:  2.0749193840381767e-07\n",
      "w6:  1.5227320908317244e-07\n",
      "w7:  1.523022920908491e-07\n",
      "w8:  1.0788927388902842e-07\n",
      "Error:  40.0\n",
      "Accuracy:  60.0\n"
     ]
    }
   ],
   "source": [
    "z = testingX.dot(theta)\n",
    "predictedY = 1.0/(1.0 + np.exp(-z))\n",
    "predictedY = np.round(predictedY)\n",
    "difference = np.abs(np.subtract(predictedY,testingY))\n",
    "temp = np.sum(difference)\n",
    "temp = temp/predictedY.shape[0]\n",
    "error = np.multiply(temp,100)\n",
    "print('Batch Gradient Descent Algorithm After Adding Features with Regularization')\n",
    "print('w0: ', theta[0])\n",
    "print('w1: ', theta[1])\n",
    "print('w2: ', theta[2])\n",
    "print('w3: ', theta[3])\n",
    "print('w4: ', theta[4])\n",
    "print('w5: ', theta[5])\n",
    "print('w6: ', theta[6])\n",
    "print('w7: ', theta[7])\n",
    "print('w8: ', theta[8])\n",
    "print('Error: ', error)\n",
    "accuracy = 100 - error\n",
    "print('Accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Stochastic Gradient Descent Algorithm with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(trainingX.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "costList,theta = cd.stochastic_gradient_descent_reg(trainingX,trainingY, theta, 2000, 0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent Algorithm After Adding Features with Regularization\n",
      "w0:  1.8370681426680204e-07\n",
      "w1:  2.3971760233104685e-07\n",
      "w2:  2.249566085976229e-07\n",
      "w3:  1.9276577355681205e-07\n",
      "w4:  1.7934848748246673e-07\n",
      "w5:  1.8844989257854825e-07\n",
      "w6:  1.3828823120750898e-07\n",
      "w7:  1.381541802875514e-07\n",
      "w8:  9.786737537366327e-08\n",
      "Error:  40.0\n",
      "Accuracy:  60.0\n"
     ]
    }
   ],
   "source": [
    "z = testingX.dot(theta)\n",
    "predictedY = 1.0/(1.0 + np.exp(-z))\n",
    "predictedY = np.round(predictedY)\n",
    "difference = np.abs(np.subtract(predictedY,testingY))\n",
    "temp = np.sum(difference)\n",
    "temp = temp/predictedY.shape[0]\n",
    "error = np.multiply(temp,100)\n",
    "print('Stochastic Gradient Descent Algorithm After Adding Features with Regularization')\n",
    "print('w0: ', theta[0])\n",
    "print('w1: ', theta[1])\n",
    "print('w2: ', theta[2])\n",
    "print('w3: ', theta[3])\n",
    "print('w4: ', theta[4])\n",
    "print('w5: ', theta[5])\n",
    "print('w6: ', theta[6])\n",
    "print('w7: ', theta[7])\n",
    "print('w8: ', theta[8])\n",
    "print('Error: ', error)\n",
    "accuracy = 100 - error\n",
    "print('Accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Mini Batch Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(trainingX.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "costList,theta = cd.mini_batch_gradient_descent_reg(trainingX,trainingY, theta, 2000, 0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini Batch Gradient Descent Algorithm After Adding Features with Regularization\n",
      "w0:  1.980136764150139e-07\n",
      "w1:  2.606879058261524e-07\n",
      "w2:  2.4484213132378397e-07\n",
      "w3:  2.098115923595527e-07\n",
      "w4:  1.9542581343435646e-07\n",
      "w5:  2.0572451273166733e-07\n",
      "w6:  1.5097611302648572e-07\n",
      "w7:  1.5100499091589825e-07\n",
      "w8:  1.0697026069396916e-07\n",
      "Error:  40.0\n",
      "Accuracy:  60.0\n"
     ]
    }
   ],
   "source": [
    "z = testingX.dot(theta)\n",
    "predictedY = 1.0/(1.0 + np.exp(-z))\n",
    "predictedY = np.round(predictedY)\n",
    "difference = np.abs(np.subtract(predictedY,testingY))\n",
    "temp = np.sum(difference)\n",
    "temp = temp/predictedY.shape[0]\n",
    "error = np.multiply(temp,100)\n",
    "print('Mini Batch Gradient Descent Algorithm After Adding Features with Regularization')\n",
    "print('w0: ', theta[0])\n",
    "print('w1: ', theta[1])\n",
    "print('w2: ', theta[2])\n",
    "print('w3: ', theta[3])\n",
    "print('w4: ', theta[4])\n",
    "print('w5: ', theta[5])\n",
    "print('w6: ', theta[6])\n",
    "print('w7: ', theta[7])\n",
    "print('w8: ', theta[8])\n",
    "print('Error: ', error)\n",
    "accuracy = 100 - error\n",
    "print('Accuracy: ',accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
